---
title: "Yushi's Teaching Experience"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  html_document:
    code_folding: show
    highlight: tango
    keep_med: true
    number_sections: false
    theme: journal
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
      toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##  Courses at Princeton as an AI {.tabset .tabset-fade .tabset-pills}

### QCB508 / QCB408 / Foundations of Statistical Genomics

    Course ID: QCB 508 / QCB 408 
    
    Title: Foundations of Statistical Genomics
    
    Semester: 2022 Spring
    
    Instructor: Dr. John D. Storey
    
#### 3.1 Course Website: [Canvas](https://princeton.instructure.com/courses/7114)

#### 3.2 Office Hour Recap

*Preface: This is a brief summary based on the conversations during Yushi's office hour in Spring 2022. Due to the pandemic, we needed to hold our office hours virtually for this semester. I was unable to provide detailed proof or answers without a blackboard. I listed these details here and I hope that they can help our friends to learn something.*

**Mar 15, 2022**

$\underline{Understand\,\,distribution\,\,of\,\,a\,\,random\,\,variable\,\,by\,\,integrals}$

Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $(E,\mathcal{E})$ be a measurable space. Expectations of random variable $X:\Omega\mapsto E$ can be interpreted as integrals with respect to $\mathbb{P}$. Let $\mu$ be the distribution of $X$. We can interpret $\mu$ as the image of $\mathbb{P}$ under $X$. For event $A\in \mathcal{F}$, we write the probability that $X$ is in $A$ as \begin{equation}
\mu(A) = \mathbb{P}(X^{-1}A) = \mathbb{P}(X\in A).
\end{equation} Note that $X^{-1}A$ means that $X$ is measurable relative to $\mathcal{F}$ and $\mathcal{E}$, i.e., the event for every $A$ in $\mathcal{E}$ is written as \begin{equation}
X^{-1}A = \{X\in A\} = \{\omega \in \Omega:X(\omega) \in A\}
\end{equation}

$\underline{Theorem}$. Let function $f:E\mapsto \Omega$ be measurable relative to $\mathcal{E}$ and $\mathcal{F}$. We define the composition $Y=f\circ X$, which is a random variable taking values in $(\Omega, \mathcal{F})$ by \begin{equation} 
Y(\omega) = f\circ X(\omega) = f(X(\omega)),\,\,\,\,\,\,\text{for}\,\,\,\omega \in \Omega.
\end{equation} If $f$ is positive $\mathcal{E}$-measurable, then \begin{equation}
\mathbb{E}[f\circ X] = \mu f.
\end{equation}

$\underline{Corollary}$. Define the indicator function \begin{equation}\mathbb{1}_{A} = \begin{cases} 1, \,\,\,\text{if } X\in A,\\0, \,\,\,\text{if } X\notin A. \end{cases} \end{equation} Notice that $\mathbb{1}_{A}$ is positive $\mathcal{E}$-measurable. Let $f=\mathbb{1}_{A}$, then we write $\mu$, the distribution of $X$, as \begin{equation}
\mu(A) = \mu \mathbb{1}_{A} = \mathbb{E}[\mathbb{1}_{A}\circ X] = \mathbb{P}(X\in A) \cdot 1 + \mathbb{P}(X\notin A) \cdot 0 = \mathbb{P}(X\in A),
\end{equation} which is the integral understanding of the distribution of a random variable. A useful application of this corollary is to calculate the p.d.f., i.e., to characterize the distribution, of a random variable when having prior information regarding the target random variable conditioning on other variables. Let us take a look at the following example. 

$\underline{Example\,\,(Distribution\,\,of\,\,random\,\,genotype)}.$ Let $X$ be the random genotype in the random-allele-frequency model. We want to characterize the distribution of $X$ but the only information we know is the conditional distribution of $X$ given individual-allele-frequency $\pi$, written as $X|\pi \sim \text{Binomial}(2,\pi)$. Here $\pi$ comes from some other distributions. A strong candidate for this is BN$(p,f)$. We can use the above corollary to derive the distribution of $X$. There are three possible outcomes of $X$ as 0, 1, or 2. For event $A=\{\omega\in \Omega: X(\omega)=0\}$, we have \begin{align}
\mathbb{P}(X=0) &= \mathbb{P}(A)= \mathbb{E}[\mathbb{P}(A|\pi)] = \mathbb{E}\big[\mathbb{E}[\mathbb{1}_{A}|\pi]\big] \nonumber \\
&= \mathbb{E}\big[\mathbb{E}[\mathbb{1}_{X=0}|\pi]\big] = \mathbb{E}\big[(1-\pi)^2\big] = \cdots
\end{align} The remaining path can be walked through by following $\mathbb{E}[Z^2]=(\mathbb{E}[Z])^2+\mathbb{V}(Z)$ while taking $Z=1-\pi$. The same strategy yields $\mathbb{P}(X=1)$ and $\mathbb{P}(X=2)$.

**Feb 24, 2022**

$\underline{Conditional\,\,distribution\,\,to\,\,model\,\,the\,\,allele\,\,evolution\,\,in\,\,finite\,\,population}$

![](./qcb508_fig1.png){width=96%}


**Feb 22, 2022**

$\underline{Basics\,\,of\,\,IBD\,\,model:\,\,dissemble\,\,genotype\,\,into\,\,alleles}$

Let $X$ and $Y$ be two genotypes at the same loci drawn randomly from a population under the identical-by-descent (IBD) model with allele frequency $p$ and inbreeding coefficient $f$. We are interested in evaluating the covariance between genotype $X$ and $Y$. Recall that IBD is defined based on alleles, i.e., the probability that two alleles are IBD. Hence a basic setup is to decompose $X$ and $Y$ into alleles level. Suppose $X$ is supported by two alleles $X_1$ and $X_2$, $Y$ is supported by two alleles $Y_1$ and $Y_2$. Suppose $X_1$ (and $Y_1$) comes from the maternal side and $X_2$ (and $Y_2$) comes from the paternal side. We write $$X=X_1+X_2,\,\,\,\,\,\,Y=Y_1+Y_2,$$ thereby deriving their covariance 
\begin{align}
\text{Cov}(X,Y) &= \text{Cov}(X_1+X_2, Y_1+Y_2) \nonumber \\
&= \mathbb{E}[(X_1+X_2)(Y_1+Y_2)]-\mathbb{E}[X_1+X_2]\mathbb{E}[Y_1+Y_2] \nonumber \\
&= \mathbb{E}[X_1Y_1]+\mathbb{E}[X_1Y_2]+\mathbb{E}[X_2Y_1]+\mathbb{E}[X_2Y_2] - \mathbb{E}[X_1]\mathbb{E}[Y_1]-\mathbb{E}[X_1]\mathbb{E}[Y_2]-\mathbb{E}[X_2]\mathbb{E}[Y_1]-\mathbb{E}[X_2]\mathbb{E}[Y_2] \nonumber \\
&= (\mathbb{E}[X_1Y_1]-\mathbb{E}[X_1]\mathbb{E}[Y_1]) + (\mathbb{E}[X_1Y_2]-\mathbb{E}[X_1]\mathbb{E}[Y_2]) + (\mathbb{E}[X_2Y_1]-\mathbb{E}[X_2]\mathbb{E}[Y_1]) + (\mathbb{E}[X_2Y_2]-\mathbb{E}[X_2]\mathbb{E}[Y_2]) \nonumber \\
&= \text{Cov}(X_1,Y_1) + \text{Cov}(X_1,Y_2) + \text{Cov}(X_2,Y_1) + \text{Cov}(X_2,Y_2) = \sum_{j=1}^2 \sum_{i=1}^2 \text{Cov}(X_i,Y_j) \nonumber \\
&= 4\text{Cov}(X_1,Y_1).
\end{align}

The last line holds since all 4 terms of covariance in line 5 are equal.

**Feb 15, 2022**

$\underline{Hints\,\,for\,\,the\,\,law\,\,of\,\,total\,\,variance}$

Let us begin by proving a simple version of the law of total variance

\begin{align}
\mathbb{V}(X) &= \mathbb{E}\big[(X-\mathbb{E}[X])^2\big] \nonumber \\
&= \mathbb{E}\big[(X-\mathbb{E}[X|Y]+\mathbb{E}[X|Y]-\mathbb{E}[X])^2\big] \nonumber \\
&= \Big\{\mathbb{E}\big[(X-\mathbb{E}[X|Y])^2\big] + 2\mathbb{E}\big[(X-\mathbb{E}[X|Y])(\mathbb{E}[X|Y]-\mathbb{E}[X])\big] \Big\}+ \mathbb{E}\big[(\mathbb{E}[X|Y]-\mathbb{E}[X])^2\big] \nonumber \\
&= \Big\{\mathbb{E}[X^2]-\mathbb{E}[(\mathbb{E}[X|Y])^2]\Big\} + \mathbb{E}\big[(\mathbb{E}[X|Y]-\mathbb{E}[\mathbb{E}[X|Y]])^2\big] \nonumber \\
&= \mathbb{E}\big[\mathbb{E}[X^2|Y]-(\mathbb{E}[X|Y])^2\big] + \mathbb{V}(\mathbb{E}[X|Y]) \nonumber \\
&= \mathbb{E}[\mathbb{V}(X|Y)] + \mathbb{V}(\mathbb{E}[X|Y]).
\end{align}

Line 1 (expand the variance/covariance term by definition) and line 2 (little trick to introduce the conditional term) may provide hints for proving the following equation 

\begin{equation}
\text{Cov}(X,Y)=\mathbb{E}[\text{Cov}(X,Y|Z)]+\text{Cov}(\mathbb{E}[X|Z],\mathbb{E}[Y|Z]).
\end{equation}

More specifically, try to think about two questions: 1) what is the math definition of $\text{Cov}(X,Y)$; 2) how to introduce the conditional terms into the expanded version of $\text{Cov}(X,Y)$. More details for deriving the first term in line 4 from line 3 

\begin{align}
\mathbb{E}\big[(X-\mathbb{E}[X|Y])\big]&=\mathbb{E}[X^2]-2\mathbb{E}\big[X\mathbb{E}[X|Y]\big]+\mathbb{E}\big[(\mathbb{E}[X|Y])^2\big] \nonumber \\
2\mathbb{E}\big[(X-\mathbb{E}[X|Y])(\mathbb{E}[X|Y]-\mathbb{E}[X])\big] &= 2\mathbb{E}\big[X\mathbb{E}[X|Y]\big]-2\mathbb{E}\big[XE[X]\big]-2\mathbb{E}\big[(\mathbb{E}[X|Y])^2\big]+2\mathbb{E}\big[\mathbb{E}[X|Y]\mathbb{E}[X]\big] \nonumber \\
&= 2\mathbb{E}\big[X\mathbb{E}[X|Y]\big]-2(\mathbb{E}[X])^2-2\mathbb{E}\big[(\mathbb{E}[X|Y])^2\big]+2(\mathbb{E}[X])^2 \nonumber \\
&= 2\mathbb{E}\big[X\mathbb{E}[X|Y]\big]-2\mathbb{E}\big[(\mathbb{E}[X|Y])^2\big] \nonumber \\
\Rightarrow \mathbb{E}\big[(X-\mathbb{E}[X|Y])\big]+ 2\mathbb{E}\big[(X-\mathbb{E}[X|Y])(\mathbb{E}[X|Y]-\mathbb{E}[X])\big] &= \mathbb{E}[X^2]-\mathbb{E}\big[(\mathbb{E}[X|Y])^2\big].
\end{align}

##  Courses at Harvard as a TA {.tabset .tabset-fade .tabset-pills}

### STAT215 / STAT115 / BST282 / Introduction to Computational Biology and Bioinformatics

    Course ID: STAT 215 / STAT 115 / BST 282
    
    Title: Introduction to Computational Biology and Bioinformatics
    
    Semester: 2019 Spring
    
    Instructor: Dr. X. Shirley Liu


#### 2.1 Course Website: [Canvas](https://canvas.harvard.edu/courses/49497)

#### 2.2 Evaluation: [Students' comments](https://github.com/YushiFT/yushift.github.io/tree/master/en/teaching) about Prof. Liu and me 

---------------------------------------------------

![](./stat115_eva.png){width=100%}


---------------------------------------------------

#### 2.3 Chapters Instructed by Me:

###### Lab 5: Bioinformatic Tools for RNA-seq Analysis, DESeq2, and Cluster Computing ( [video](https://www.youtube.com/watch?v=mz64t8-xWgg&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=3&t=4s) and [material](https://github.com/stat115/Lab5) )


###### Lab 6: Single Cell RNA-seq Analysis and Optimizing I/O Performance on the Cluster ( [video 1](https://www.youtube.com/watch?v=8lZXWAmg9sE&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=3), [video 2](https://www.youtube.com/watch?v=CidSoNLWXUg&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=4) and [material](https://github.com/stat115/Lab6) )


###### Lab 7: ChIP-seq data analysis, Bedtools, UCSC Genome Browser, Cistrome and Odyssey ( [video](https://www.youtube.com/watch?v=US5DPGkUBbQ&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=5) and [material](https://github.com/stat115/Lab7) )


###### Lab 8: BETA, VCF, ChIP-seq Motifs Analysis ( [video](https://www.youtube.com/watch?v=QbIXoj2Us1w&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=6) and [material](https://github.com/stat115/Lab8) )


###### Lab 9: Hidden Markov Model and TCGA Data ( [video 1](https://www.youtube.com/watch?v=Z-mC-8RDD9U&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=7), [video 2](https://www.youtube.com/watch?v=_8FrxmIYVK4&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=8) and [material](https://github.com/stat115/Lab9) )


###### Lab 10: GWAS, HaploReg, RegulomeDB, Regression, and Feature Selection ( [video](https://www.youtube.com/watch?v=48coCc6RXzI&list=PL1LC4fx-wvLcqOtVMp48f9kVaGA9JQLLP&index=9) and [material](https://github.com/stat115/Lab10) )


-------------------------------------------

### BST215 / Linear and Longitudinal Regression


    Course ID: BST 215
    
    Title: Linear and Longitudinal Regression
    
    Semester: 2018 Summer
    
    Instructor: Dr. Garrett Fitzmaurice


#### 1.1 Course Website: [Canvas](https://canvas.harvard.edu/courses/41831)

#### 1.2 Evaluation: [Students' comments](https://github.com/YushiFT/yushift.github.io/blob/master/en/teaching/BST215_Evaluation.pdf) about Prof. Fitzmaurice and me 

-------------------------------------

![](./bst215_eva.png){width=100%}

------------------------------------------------


## [Back to Home](https://yushift.github.io)
[Homepage](https://yushift.github.io)


